{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Consider text paragraph. \"\"\"Hello all, Welcome to Python Programming Academy. Python\n",
        "Programming Academy is a nice platform to learn new programming skills. It is difficult to get enrolled\n",
        "in this Academy.\"\"\" Preprocess the text to remove any special characters and digits. Generate the\n",
        "summary using extractive summarization process"
      ],
      "metadata": {
        "id": "5MapB-7KhwLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "svk7Wg1d5S5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8axccKm5UP-",
        "outputId": "be7d811e-586f-45e1-f4df-34e8b8700f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI_N0j1r5Ul2",
        "outputId": "1a2ab466-7482-4b11-924d-6b2e8c1fee09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vLVqRT35U42",
        "outputId": "c10bcaa8-38be-4c18-dc3a-499c49ad4948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_mQzWnmDhsaz",
        "outputId": "25d32c11-60c6-4012-c7cf-137f7ef2bb4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Python Programming Academy is a nice platform to learn new programming skills. It is difficult to get enrolled in this Academy. Hello all, Welcome to Python Programming Academy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize ,sent_tokenize\n",
        "import re\n",
        "\n",
        "def get_preprocess_text(text):\n",
        "    processed_text = re.sub(r'[^a-zA-Z\\s]' , '' ,text)\n",
        "    processed_text = re.sub(r'\\d+' ,'',processed_text)\n",
        "    return processed_text\n",
        "\n",
        "text =\"\"\"Hello all, Welcome to Python Programming Academy. Python Programming Academy is a nice platform to learn new programming skills. It is difficult to get enrolled in this Academy.\"\"\"\n",
        "processed_text = get_preprocess_text(text)\n",
        "processed_text\n",
        "\n",
        "stopWords  = set(stopwords.words(\"english\"))\n",
        "\n",
        "words = word_tokenize(processed_text)\n",
        "\n",
        "word_freq = {}\n",
        "for word in words:\n",
        "    if word in stopWords:\n",
        "        continue\n",
        "    if word in word_freq:\n",
        "        word_freq[word] +=1\n",
        "    else:\n",
        "        word_freq[word] = 1\n",
        "\n",
        "maximum_frq = max(word_freq.values())\n",
        "\n",
        "for word in word_freq.keys():\n",
        "    word_freq[word] = (word_freq[word]/maximum_frq)\n",
        "\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "sentenceValues = {}\n",
        "for sentence in sentences:\n",
        "    for word,frq in word_freq.items():\n",
        "        if word in sentence.lower():\n",
        "            if sentence in sentenceValues:\n",
        "                sentenceValues[sentence] += frq\n",
        "            else:\n",
        "                sentenceValues[sentence] = frq\n",
        "\n",
        "\n",
        "import heapq\n",
        "summary = ''\n",
        "summary_sentences = heapq.nlargest(10, sentenceValues , key = sentenceValues.get)\n",
        "summary = ' '.join(summary_sentences)\n",
        "summary"
      ]
    }
  ]
}